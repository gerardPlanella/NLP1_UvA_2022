{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gerardPlanella/NLP1_UvA_2022/blob/main/Assignment1/NLP1_2022_Practical_1_(student_version).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-aRiOgl4nHg"
      },
      "source": [
        "------\n",
        "**You cannot save any changes you make to this file, so please make sure to save it on your Google Colab drive or download it as a .ipynb file.**\n",
        "\n",
        "------\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIZrAUx57vsM"
      },
      "source": [
        "Practical 1: Sentiment Detection in Movie Reviews\n",
        "========================================\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4kXPMhyngZW"
      },
      "source": [
        "This practical concerns detecting sentiment in movie reviews. This is a typical NLP classification task.\n",
        "In [this file](https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json) (80MB) you will find 1000 positive and 1000 negative **movie reviews**.\n",
        "Each review is a **document** and consists of one or more sentences.\n",
        "\n",
        "To prepare yourself for this practical, you should\n",
        "have a look at a few of these texts to understand the difficulties of\n",
        "the task: how might one go about classifying the texts? You will write\n",
        "code that decides whether a movie review conveys positive or\n",
        "negative sentiment.\n",
        "\n",
        "Please make sure you have read the following paper:\n",
        "\n",
        ">   Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan\n",
        "(2002). \n",
        "[Thumbs up? Sentiment Classification using Machine Learning\n",
        "Techniques](https://dl.acm.org/citation.cfm?id=1118704). EMNLP.\n",
        "\n",
        "Bo Pang et al. introduced the movie review sentiment\n",
        "classification task, and the above paper was one of the first papers on\n",
        "the topic. The first version of your sentiment classifier will do\n",
        "something similar to Pang et al.'s system. If you have questions about it,\n",
        "you should resolve you doubts as soon as possible with your TA.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb7errgRASzZ"
      },
      "source": [
        "**Advice**\n",
        "\n",
        "Please read through the entire practical and familiarise\n",
        "yourself with all requirements before you start coding or otherwise\n",
        "solving the tasks. Writing clean and concise code can make the difference\n",
        "between solving the assignment in a matter of hours, and taking days to\n",
        "run all experiments.\n",
        "\n",
        "## Environment\n",
        "\n",
        "All code should be written in **Python 3**. \n",
        "This is the default in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaZnxptMJiD7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "263d49d8-916d-4f4c-e81b-9a9bda43bb5a"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYZyIF7lJnGn"
      },
      "source": [
        "If you want to run code on your own computer, then download this notebook through `File -> Download .ipynb`.\n",
        "The easiest way to\n",
        "install Python is through downloading\n",
        "[Anaconda](https://www.anaconda.com/download). \n",
        "After installation, you can start the notebook by typing `jupyter notebook filename.ipynb`.\n",
        "You can also use an IDE\n",
        "such as [PyCharm](https://www.jetbrains.com/pycharm/download/) to make\n",
        "coding and debugging easier. It is good practice to create a [virtual\n",
        "environment](https://docs.python.org/3/tutorial/venv.html) for this\n",
        "project, so that any Python packages don’t interfere with other\n",
        "projects. \n",
        " \n",
        "\n",
        "**Learning Python 3**\n",
        "\n",
        "If you are new to Python 3, you may want to check out a few of these resources:\n",
        "- https://learnxinyminutes.com/docs/python3/\n",
        "- https://www.learnpython.org/\n",
        "- https://docs.python.org/3/tutorial/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hok-BFu9lGoK"
      },
      "source": [
        "import math\n",
        "import os\n",
        "import sys\n",
        "from subprocess import call\n",
        "from nltk import FreqDist\n",
        "from nltk.util import ngrams\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import sklearn as sk\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "import json\n",
        "from collections import Counter\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#Added libraries\n",
        "from collections import defaultdict\n",
        "import functools\n",
        "from pandas import DataFrame\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXWyGHwE-ieQ"
      },
      "source": [
        "## Loading the data\n",
        "\n",
        "**Download the sentiment lexicon and the movie reviews dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm-rakqtlMOT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "360460db-f784-42c1-c2da-53dd1e74b213"
      },
      "source": [
        "# download sentiment lexicon\n",
        "!wget https://gist.githubusercontent.com/bastings/d6f99dcb6c82231b94b013031356ba05/raw/f80a0281eba8621b122012c89c8b5e2200b39fd6/sent_lexicon\n",
        "# download review data\n",
        "!wget https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-15 10:55:56--  https://gist.githubusercontent.com/bastings/d6f99dcb6c82231b94b013031356ba05/raw/f80a0281eba8621b122012c89c8b5e2200b39fd6/sent_lexicon\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 662577 (647K) [text/plain]\n",
            "Saving to: ‘sent_lexicon.1’\n",
            "\n",
            "sent_lexicon.1      100%[===================>] 647.05K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2022-11-15 10:55:58 (152 MB/s) - ‘sent_lexicon.1’ saved [662577/662577]\n",
            "\n",
            "--2022-11-15 10:55:58--  https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 83503869 (80M) [text/plain]\n",
            "Saving to: ‘reviews.json.1’\n",
            "\n",
            "reviews.json.1      100%[===================>]  79.63M   337MB/s    in 0.2s    \n",
            "\n",
            "2022-11-15 10:56:03 (337 MB/s) - ‘reviews.json.1’ saved [83503869/83503869]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkPwuHp5LSuQ"
      },
      "source": [
        "**Load the movie reviews.**\n",
        "\n",
        "Each word in a review comes with its part-of-speech tag. For documentation on POS-tags, see https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "careEKj-mRpl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "63317c43-99b9-474e-c2eb-0f3aa375b98f"
      },
      "source": [
        "# file structure:\n",
        "# [\n",
        "#  {\"cv\": integer, \"sentiment\": str, \"content\": list} \n",
        "#  {\"cv\": integer, \"sentiment\": str, \"content\": list} \n",
        "#   ..\n",
        "# ]\n",
        "# where `content` is a list of sentences, \n",
        "# with a sentence being a list of (token, pos_tag) pairs.\n",
        "\n",
        "\n",
        "with open(\"reviews.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  reviews = json.load(f)\n",
        "  \n",
        "print(\"Total number of reviews:\", len(reviews), '\\n')\n",
        "\n",
        "def print_sentence_with_pos(s):\n",
        "  print(\" \".join(\"%s/%s\" % (token, pos_tag) for token, pos_tag in s))\n",
        "\n",
        "for i, r in enumerate(reviews):\n",
        "  \"\"\"\n",
        "  print(r[\"cv\"], r[\"sentiment\"], len(r[\"content\"]))  # cv, sentiment, num sents\n",
        "  print(r[\"content\"])\n",
        "  \"\"\"\n",
        "  print_sentence_with_pos(r[\"content\"][0])\n",
        "  if i == 4: \n",
        "    break\n",
        "    \n",
        "pos_tags = []\n",
        "c = Counter()\n",
        "for review in reviews:\n",
        "  for sentence in review[\"content\"]:\n",
        "    for token, pos_tag in sentence:\n",
        "      c[token.lower()] += 1\n",
        "      if pos_tag not in pos_tags :\n",
        "        pos_tags.append(pos_tag)\n",
        "\n",
        "\"\"\"\n",
        "print(\"\\nNumber of word types:\", len(c))\n",
        "print(\"Number of word tokens:\", sum(c.values()))\n",
        "\n",
        "print(\"\\nMost common tokens:\")\n",
        "for token, count in c.most_common(20):\n",
        "  print(\"%10s : %8d\" % (token, count))\n",
        "\n",
        "print(\"Pos Tags: \" + str(pos_tags))\n",
        "\"\"\"\n",
        "\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of reviews: 2000 \n",
            "\n",
            "Two/CD teen/JJ couples/NNS go/VBP to/TO a/DT church/NN party/NN ,/, drink/NN and/CC then/RB drive/NN ./.\n",
            "Damn/JJ that/IN Y2K/CD bug/NN ./.\n",
            "It/PRP is/VBZ movies/NNS like/IN these/DT that/WDT make/VBP a/DT jaded/JJ movie/NN viewer/NN thankful/JJ for/IN the/DT invention/NN of/IN the/DT Timex/NNP IndiGlo/NNP watch/NN ./.\n",
            "QUEST/NN FOR/IN CAMELOT/NNP ``/`` Quest/NNP for/IN Camelot/NNP ''/'' is/VBZ Warner/NNP Bros./NNP '/POS first/JJ feature-length/JJ ,/, fully-animated/JJ attempt/NN to/TO steal/VB clout/NN from/IN Disney/NNP 's/POS cartoon/NN empire/NN ,/, but/CC the/DT mouse/NN has/VBZ no/DT reason/NN to/TO be/VB worried/VBN ./.\n",
            "Synopsis/NNPS :/: A/DT mentally/RB unstable/JJ man/NN undergoing/VBG psychotherapy/NN saves/VBZ a/DT boy/NN from/IN a/DT potentially/RB fatal/JJ accident/NN and/CC then/RB falls/VBZ in/IN love/NN with/IN the/DT boy/NN 's/POS mother/NN ,/, a/DT fledgling/NN restauranteur/NN ./.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(\"\\nNumber of word types:\", len(c))\\nprint(\"Number of word tokens:\", sum(c.values()))\\n\\nprint(\"\\nMost common tokens:\")\\nfor token, count in c.most_common(20):\\n  print(\"%10s : %8d\" % (token, count))\\n\\nprint(\"Pos Tags: \" + str(pos_tags))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6PWaEoh8B34"
      },
      "source": [
        "#(1) Lexicon-based approach (3.5pts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsTSMb6ma4E8"
      },
      "source": [
        "A traditional approach to classify documents according to their sentiment is the lexicon-based approach. To implement this approach, you need a **sentiment lexicon**, i.e., a list of words annotated with a sentiment label (e.g., positive and negative, or a score from 0 to 5).\n",
        "\n",
        "In this practical, you will use the sentiment\n",
        "lexicon released by Wilson et al. (2005).\n",
        "\n",
        "> Theresa Wilson, Janyce Wiebe, and Paul Hoffmann\n",
        "(2005). [Recognizing Contextual Polarity in Phrase-Level Sentiment\n",
        "Analysis](http://www.aclweb.org/anthology/H/H05/H05-1044.pdf). HLT-EMNLP.\n",
        "\n",
        "Pay attention to all the information available in the sentiment lexicon. The field *word1* contains the lemma, *priorpolarity* contains the sentiment label (positive, negative, both, or neutral), *type* gives you the magnitude of the word's sentiment (strong or weak), and *pos1* gives you the part-of-speech tag of the lemma. Some lemmas can have multiple part-of-speech tags and thus multiple entries in the lexicon. The path of the lexicon file is `\"sent_lexicon\"`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ogq0Eq2hQglh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68f94e10-9f78-47b5-b384-ba71f684c4a6"
      },
      "source": [
        "with open(\"sent_lexicon\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  pos_tags = []\n",
        "  types = []\n",
        "  mags = []\n",
        "  polarities = []\n",
        "  line_cnt = 0\n",
        "  for line in f:\n",
        "    line_cnt += 1\n",
        "    if(line_cnt == 1):\n",
        "      print(\"First line: \" + line)\n",
        "    word = line.strip().split(\" \")[2].split(\"=\")[1]\n",
        "    pos_tag = line.strip().split(\" \")[3].split(\"=\")[1]\n",
        "    mag = line.strip().split(\" \")[0].split(\"=\")[1]\n",
        "    polarity = line.strip().split(\" \")[5].split(\"=\")[1]\n",
        "    if  pos_tag not in pos_tags:\n",
        "      pos_tags.append(pos_tag)\n",
        "    if mag not in mags:\n",
        "      mags.append(mag)\n",
        "    if polarity not in polarities:\n",
        "      polarities.append(polarity)\n",
        "    \n",
        "\n",
        "  print(\"Mags:\")\n",
        "  print(mags)\n",
        "  print(\"POS Tags:\")\n",
        "  print(pos_tags)\n",
        "  print(\"Polarities: \")\n",
        "  print(polarities)\n",
        "  print(\"Lines:\")\n",
        "  print(line_cnt)\n",
        "\n",
        "  "
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First line: type=weaksubj len=1 word1=abandoned pos1=adj stemmed1=n priorpolarity=negative\n",
            "\n",
            "Mags:\n",
            "['weaksubj', 'strongsubj']\n",
            "POS Tags:\n",
            "['adj', 'noun', 'verb', 'anypos', 'adverb']\n",
            "Polarities: \n",
            "['negative', 'positive', 'neutral', 'both']\n",
            "Lines:\n",
            "8222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mml4nOtIUBhn"
      },
      "source": [
        "Lexica such as this can be used to solve\n",
        "the classification task without using Machine Learning. For example, one might look up every word $w_1 ... w_n$ in a document, and compute a **binary score**\n",
        "$S_{binary}$ by counting how many words have a positive or a\n",
        "negative label in the sentiment lexicon $SLex$.\n",
        "\n",
        "$$S_{binary}(w_1 w_2 ... w_n) = \\sum_{i = 1}^{n}\\text{sign}(SLex\\big[w_i\\big])$$\n",
        "\n",
        "where $\\text{sign}(SLex\\big[w_i\\big])$ refers to the polarity of $w_i$.\n",
        "\n",
        "**Threshold.** On average, there are more positive than negative words per review (~7.13 more positive than negative per review) to take this bias into account you should use a threshold of **8** (roughly the bias itself) to make it harder to classify as positive.\n",
        "\n",
        "$$\n",
        "\\text{classify}(S_{binary}(w_1 w_2 ... w_n)) = \\bigg\\{\\begin{array}{ll}\n",
        "        \\text{positive} & \\text{if } S_{binary}(w_1w_2...w_n) > threshold\\\\\n",
        "        \\text{negative} & \\text{otherwise}\n",
        "        \\end{array}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOFnMvbeeZrc"
      },
      "source": [
        "#### (Q1.1) Implement this approach and report its classification accuracy. (1 pt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED2aTEYutW1-"
      },
      "source": [
        "class WordInfo():\n",
        "  def __init__(self, word, polarity, weight, pos_tag):\n",
        "    self.polarity = polarity\n",
        "    self.word = word\n",
        "    self.weight = weight\n",
        "    self.pos_tag = pos_tag\n",
        "  \n",
        "  @classmethod\n",
        "  def extractWord(cls, in_str):\n",
        "    params = in_str.split(\" \")\n",
        "    priorpolarity = params[5].split(\"=\")[1].lower()\n",
        "\n",
        "    if priorpolarity == \"negative\":\n",
        "      polarity = -1\n",
        "    elif priorpolarity == \"positive\":\n",
        "      polarity = 1\n",
        "    elif priorpolarity == \"neutral\":\n",
        "      polarity = 0\n",
        "    else:\n",
        "      polarity = 0\n",
        "\n",
        "    word = params[2].split(\"=\")[1]\n",
        "    weight = params[0].split(\"=\")[1]\n",
        "\n",
        "    if(weight.lower() == \"weaksubj\"):\n",
        "      weight = 1\n",
        "    else:\n",
        "      weight = 2\n",
        "\n",
        "    pos_tag = params[3].split(\"=\")[1]\n",
        "\n",
        "    if pos_tag == \"adj\":\n",
        "      pos_tag = \"JJ\"\n",
        "    elif pos_tag == \"noun\":\n",
        "      pos_tag = \"NN\"\n",
        "    elif pos_tag == \"verb\":\n",
        "      pos_tag = \"VB\"\n",
        "    elif pos_tag == \"adverb\":\n",
        "      pos_tag = \"RB\"\n",
        "\n",
        "\n",
        "    return cls(word, polarity, weight, pos_tag)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "class Lexicon():\n",
        "  __slots__ = [\"wordDict\"]\n",
        "\n",
        "  def __init__(self, lexicon_title = \"sent_lexicon\") -> None:\n",
        "    self.wordDict = defaultdict(list)\n",
        "    with open(lexicon_title, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "      for line in f:\n",
        "        wordInfo = WordInfo.extractWord(line.strip())\n",
        "        self.wordDict[wordInfo.word].append(wordInfo)\n",
        "\n",
        "  def obtainWordInfo(self, word, pos_tag):\n",
        "    if(word not in self.wordDict.keys()):\n",
        "      return None\n",
        "    wordList = self.wordDict[word]\n",
        "    for wordInfo in wordList:\n",
        "      lex_pos = wordInfo.pos_tag\n",
        "      if(pos_tag[0:1] == lex_pos or pos_tag == lex_pos):\n",
        "        return wordInfo\n",
        "    return None\n",
        "    \n",
        "  \n",
        "  def classifyBinaryScore(self, review, threshold):\n",
        "    content = review[\"content\"]\n",
        "\n",
        "    binaryScore = 0\n",
        "    for sentence in content: \n",
        "      for word, pos_tag in sentence:\n",
        "        wordInfo = self.obtainWordInfo(word.lower(), pos_tag)\n",
        "        if(wordInfo is not None):\n",
        "          #print(f\"FOUND Word \\\"{word}\\\" with POS Tag {pos_tag} has polarity {wordInfo.polarity}\")\n",
        "          binaryScore+=wordInfo.polarity\n",
        "        else:\n",
        "          pass\n",
        "          #print(f\"Word \\\"{word}\\\" with POS Tag {pos_tag} not found in Lexicon\")\n",
        "\n",
        "    if binaryScore > threshold :\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "\n",
        "  def classifyWeightedScore(self, review, threshold):\n",
        "    content = review[\"content\"]\n",
        "\n",
        "    weightedScore = 0\n",
        "    for sentence in content: \n",
        "      for word, pos_tag in sentence:\n",
        "        wordInfo = self.obtainWordInfo(word, pos_tag)\n",
        "\n",
        "        if(wordInfo is not None):\n",
        "          #print(f\"FOUND Word \\\"{word}\\\" with POS Tag {pos_tag} has polarity {wordInfo.polarity} and weight {wordInfo.weight}\")\n",
        "          weightedScore+=(wordInfo.polarity * wordInfo.weight)\n",
        "        else:\n",
        "          pass\n",
        "          #print(f\"Word \\\"{word}\\\" with POS Tag {pos_tag} not found in Lexicon\")\n",
        "    \n",
        "    if weightedScore > threshold :\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "lexicon = Lexicon()  \n",
        "\n",
        "\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy528EUTphz5",
        "outputId": "9f7c7c10-1455-445e-d369-df50d71de4ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# token_results should be a list of binary indicators; for example [1, 0, 1, ...] \n",
        "# where 1 indicates a correct classification and 0 an incorrect classification. \n",
        "\n",
        "token_results = []\n",
        "n_correct = 0\n",
        "threshold = 4\n",
        "\n",
        "for idx, review in enumerate(reviews):\n",
        "  token_results.append(lexicon.classifyBinaryScore(review, threshold))\n",
        "  if(token_results[idx] == 1 and review[\"sentiment\"] == \"POS\") or (token_results[idx] == 0 and review[\"sentiment\"] == \"NEG\"):\n",
        "    n_correct +=1\n",
        "\n",
        "    \n",
        "token_accuracy = (n_correct / len(reviews)) * 100\n",
        "print(\"Accuracy: %0.2f\" % token_accuracy)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 66.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twox0s_3eS0V"
      },
      "source": [
        "As the sentiment lexicon also has information about the **magnitude** of\n",
        "sentiment (e.g., *“excellent\"* has the same sentiment _polarity_ as *“good\"* but it has a higher magnitude), we can take a more fine-grained approach by adding up all\n",
        "sentiment scores, and deciding the polarity of the movie review using\n",
        "the sign of the weighted score $S_{weighted}$.\n",
        "\n",
        "$$S_{weighted}(w_1w_2...w_n) = \\sum_{i = 1}^{n}SLex\\big[w_i\\big]$$\n",
        "\n",
        "\n",
        "Make sure you define an appropriate threshold for this approach.\n",
        "\n",
        "#### (Q1.2) Now incorporate magnitude information and report the classification accuracy. Don't forget to use the threshold. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG3hUDnPtkhS"
      },
      "source": [
        "lexicon = Lexicon() "
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vVk7CvDpyka",
        "outputId": "fa016064-8c02-4b93-b483-65960a72d8d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "magnitude_results = []\n",
        "\n",
        "n_correct = 0\n",
        "threshold = 3\n",
        "\n",
        "for idx, review in enumerate(reviews):\n",
        "  magnitude_results.append(lexicon.classifyWeightedScore(review, threshold))\n",
        "  if(magnitude_results[idx] == 1 and review[\"sentiment\"] == \"POS\") or (magnitude_results[idx] == 0 and review[\"sentiment\"] == \"NEG\"):\n",
        "    n_correct +=1\n",
        "\n",
        "\n",
        "magnitude_accuracy = (n_correct / len(reviews)) * 100\n",
        "print(\"Accuracy: %0.2f\" % magnitude_accuracy)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 67.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9SHoGPfsAHV"
      },
      "source": [
        "#### (Q.1.3) Make a barplot of the two results (0.5pt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LgBcYcXsEk3",
        "outputId": "d7763614-5ca0-49b3-e0f3-35b65c03dc43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "d = {'Binary Classifier': token_accuracy, 'Binary Weighted Classifier': magnitude_accuracy}\n",
        "classifiers = list(d.keys())\n",
        "acc = list(d.values())\n",
        "\n",
        "plt.bar(classifiers, acc, width = 0.2)\n",
        "plt.xlabel(\"Classifiers\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Lexicon based approach for review classification\")\n",
        "plt.show()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNhS8OCVxMHd"
      },
      "source": [
        "#### (Q1.4) A better threshold (1pt)\n",
        "Above we have defined a threshold to account for an inherent bias in the dataset: there are more positive than negative words per review.\n",
        "However, that threshold does not take into account *document length*. Explain why this is a problem and implement an alternative way to compute the threshold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo7gk1I-omLI"
      },
      "source": [
        "The fixed threshold can pose problems as depending on our dataset we would have to manually tune said threshold. Furthermore, if the length of the reviews in our dataset is not close to constant the longer reviews will tend to get higher scores than the shorter ones as they will probably contain more positive words, making them more probable to be classified as positive.\n",
        "\n",
        "A solution to this problem would be calculating a threshold that would take into account the review length, by, for example, using the length of the document as a mutiplying factor to the threshold.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dwt0B8h8aKjr",
        "outputId": "4297a178-1010-4017-fb75-a3f38ce3bea9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class ImprovedLexicon(Lexicon):\n",
        "\n",
        "  def __init__(self, lexicon_title = \"sent_lexicon\"):\n",
        "    super().__init__(lexicon_title)\n",
        "    self.THRESHOLD_FACTOR = 0.003\n",
        "    self.THRESHOLD_FACTOR_W = 0.003\n",
        "  \n",
        "\n",
        "  @staticmethod\n",
        "  def calculateNWords(review):\n",
        "    cnt = 0\n",
        "    for sentence in review[\"content\"]:\n",
        "      cnt+=len(sentence)\n",
        "    return cnt\n",
        "\n",
        "  def classifyBinaryScoreImproved(self, review):\n",
        "    improvedThreshold = self.calculateNWords(review)*self.THRESHOLD_FACTOR\n",
        "    return self.classifyBinaryScore(review, improvedThreshold)\n",
        "\n",
        "  def classifyWeightedScoreImproved(self, review):\n",
        "    improvedThreshold = self.calculateNWords(review)*self.THRESHOLD_FACTOR_W\n",
        "    return self.classifyWeightedScore(review, improvedThreshold)\n",
        "\n",
        "\n",
        "lex = ImprovedLexicon()\n",
        "\n",
        "\n",
        "magnitude_results = []\n",
        "\n",
        "n_correct = 0\n",
        "threshold = 3\n",
        "\n",
        "for idx, review in enumerate(reviews):\n",
        "  magnitude_results.append(lex.classifyWeightedScoreImproved(review))\n",
        "  if(magnitude_results[idx] == 1 and review[\"sentiment\"] == \"POS\") or (magnitude_results[idx] == 0 and review[\"sentiment\"] == \"NEG\"):\n",
        "    n_correct +=1\n",
        "\n",
        "\n",
        "magnitude_accuracy = (n_correct / len(reviews)) * 100\n",
        "print(\"Accuracy: %0.2f\" % magnitude_accuracy)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 67.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LibV4nR89BXb"
      },
      "source": [
        "# (2) Naive Bayes (9.5pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnF9adQnuwia"
      },
      "source": [
        "\n",
        "Your second task is to program a simple Machine Learning approach that operates\n",
        "on a simple Bag-of-Words (BoW) representation of the text data, as\n",
        "described by Pang et al. (2002). In this approach, the only features we\n",
        "will consider are the words in the text themselves, without bringing in\n",
        "external sources of information. The BoW model is a popular way of\n",
        "representing texts as vectors, making it\n",
        "easy to apply classical Machine Learning algorithms on NLP tasks.\n",
        "However, the BoW representation is also very crude, since it discards\n",
        "all information related to word order and grammatical structure in the\n",
        "original text—as the name suggests.\n",
        "\n",
        "## Writing your own classifier (4pts)\n",
        "\n",
        "Write your own code to implement the Naive Bayes (NB) classifier. As\n",
        "a reminder, the Naive Bayes classifier works according to the following\n",
        "equation:\n",
        "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} P(c|\\bar{f}) = \\operatorname*{arg\\,max}_{c \\in C} P(c)\\prod^n_{i=1} P(f_i|c)$$\n",
        "where $C = \\{ \\text{POS}, \\text{NEG} \\}$ is the set of possible classes,\n",
        "$\\hat{c} \\in C$ is the most probable class, and $\\bar{f}$ is the feature\n",
        "vector. Remember that we use the log of these probabilities when making\n",
        "a prediction:\n",
        "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} \\Big\\{\\log P(c) + \\sum^n_{i=1} \\log P(f_i|c)\\Big\\}$$\n",
        "\n",
        "You can find more details about Naive Bayes in [Jurafsky &\n",
        "Martin](https://web.stanford.edu/~jurafsky/slp3/). You can also look at\n",
        "this helpful\n",
        "[pseudo-code](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html).\n",
        "\n",
        "*Note: this section and the next aim to put you in a position to replicate\n",
        "    Pang et al.'s Naive Bayes results. However, your numerical results\n",
        "    will differ from theirs, as they used different data.*\n",
        "\n",
        "**You must write the Naive Bayes training and prediction code from\n",
        "scratch.** You will not be given credit for using off-the-shelf Machine\n",
        "Learning libraries.\n",
        "\n",
        "The data contains the text of the reviews, where each document consists\n",
        "of the sentences in the review, the sentiment of the review and an index\n",
        "(cv) that you will later use for cross-validation. The\n",
        "text has already been tokenised and POS-tagged for you. Your algorithm\n",
        "should read in the text, **lowercase it**, store the words and their\n",
        "frequencies in an appropriate data structure that allows for easy\n",
        "computation of the probabilities used in the Naive Bayes algorithm, and\n",
        "then make predictions for new instances.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEpyQSBSkb33"
      },
      "source": [
        "#### (Q2.1) Unseen words (1pt)\n",
        "The presence of words in the test dataset that\n",
        "have not been seen during training can cause probabilities in the Naive Bayes classifier to equal $0$.\n",
        "These can be words which are unseen in both positive and negative training reviews (case 1), but also words which are seen in reviews _of only one sentiment class_ in the training dataset (case 2). In both cases, **you should skip these words for both classes**.  What would be the problem instead with skipping words only for one class in case 2? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BanFiYYnoxDW"
      },
      "source": [
        "If we skip all the unseen words for one sentiment class, then the words with 0 count in our vocabulary will not be taken into account. This means that, if for example, we dont take into account unseen words appearing in positive reviews, we will have a lot of words missing in our negative sentiment vocabulary, which on testing data will make our classifier positively biased."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsZRhaI3WvzC"
      },
      "source": [
        "#### (Q2.2) Train your classifier on (positive and negative) reviews with cv-value 000-899, and test it on the remaining (positive and negative) reviews cv900–cv999.  Report results using classification accuracy as your evaluation metric. Your  features are the word vocabulary. The value of a feature is the count of that feature (word) in the document. (2pts)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7zaJYGFvIJ3"
      },
      "source": [
        "def separateData(reviews, training_idx_start = 0, training_idx_end = 899, testing_idx_start = 900, testing_idx_end = 999):\n",
        "  training_set = []\n",
        "  test_set = []\n",
        "  class_freq = {}\n",
        "  for review in reviews:\n",
        "    if review[\"cv\"]<= training_idx_end and review[\"cv\"] >= training_idx_start:\n",
        "      training_set.append(review)\n",
        "      if(review[\"sentiment\"] not in class_freq.keys()):\n",
        "        class_freq[review[\"sentiment\"]] = 1\n",
        "      else:\n",
        "        class_freq[review[\"sentiment\"]]+= 1\n",
        "    elif review[\"cv\"]<= testing_idx_end and review[\"cv\"] >= testing_idx_start:\n",
        "      test_set.append(review)\n",
        "      if(review[\"sentiment\"] not in class_freq.keys()):\n",
        "        class_freq[review[\"sentiment\"]] = 0 #So we have all classes in dict keys\n",
        "\n",
        "  return training_set, test_set, class_freq\n",
        "\n",
        "def trainNBClassifier(reviews, class_freq, smoothing = False, stem_func = lambda x:x, n_grams = [1]):\n",
        "  features = {}\n",
        "  N_docs = len(reviews)\n",
        "\n",
        "  vocab = {}\n",
        "\n",
        "  for idx, review in enumerate(reviews):\n",
        "    if(idx not in vocab.keys()):\n",
        "      vocab[idx] = [review[\"sentiment\"], {}]\n",
        "    for sentence in review[\"content\"]:\n",
        "      unigrams = [word[0] for word in sentence]\n",
        "      for n in n_grams:\n",
        "        if(n not in vocab[idx][1].keys()):\n",
        "          vocab[idx][1][n] = []\n",
        "        #We iterate through our list of ngranms and create our vocabulary\n",
        "        tokens = ngrams(unigrams, n)\n",
        "        token_list = [] \n",
        "        for token in tokens:\n",
        "          #Apply stemming function to tokems, default function doesn't modify them\n",
        "          stem_token = stem_func(\" \".join(token).lower())\n",
        "          token_list.append(stem_token)\n",
        "          if stem_token not in features.keys():\n",
        "            features[stem_token] = 1\n",
        "          else:\n",
        "            features[stem_token] += 1\n",
        "        vocab[idx][1][n] += token_list\n",
        "      \n",
        "\n",
        "  #Calculate Prior probabilities\n",
        "  prior = {}\n",
        "  for cl in class_freq.keys():\n",
        "    Nc = class_freq[cl]\n",
        "    prior[cl] = Nc / N_docs\n",
        "\n",
        "\n",
        "  sentimentWordCount = defaultdict(dict)\n",
        "\n",
        "  #Create frequency vocab\n",
        "  for cv in vocab:\n",
        "    sentiment = vocab[cv][0]\n",
        "    for n in vocab[cv][1].keys():\n",
        "      for word_low in vocab[cv][1][n]:\n",
        "        if word_low in sentimentWordCount[sentiment].keys():\n",
        "          sentimentWordCount[sentiment][word_low] += 1\n",
        "\n",
        "        if smoothing:\n",
        "          if word_low not in sentimentWordCount[\"NEG\"].keys():\n",
        "            sentimentWordCount[\"NEG\"][word_low] = 1 if sentiment == \"NEG\" else 0\n",
        "          if word_low not in sentimentWordCount[\"POS\"].keys():\n",
        "            sentimentWordCount[\"POS\"][word_low] = 1 if sentiment == \"POS\" else 0\n",
        "            continue\n",
        "        elif word_low not in sentimentWordCount[sentiment].keys():\n",
        "          sentimentWordCount[sentiment][word_low] = 1\n",
        "\n",
        "  #Perform intersection of both dictionaries\n",
        "  if(not smoothing):\n",
        "    words_to_remove = list(sentimentWordCount[\"POS\"].keys() ^ sentimentWordCount[\"NEG\"].keys())\n",
        "    list(map(lambda x: functools.partial(sentimentWordCount[\"POS\"].pop, x, None)(), words_to_remove))\n",
        "    list(map(lambda x: functools.partial(sentimentWordCount[\"NEG\"].pop, x, None)(), words_to_remove))\n",
        "    list(map(lambda x: functools.partial(features.pop, x, None)(), words_to_remove))  \n",
        "\n",
        "  smoothing_factor = 0 if not smoothing else 1\n",
        "\n",
        "  cond_prob = defaultdict(dict)\n",
        "\n",
        "  #Calculate conditional Probabilities\n",
        "  for sent in sentimentWordCount.keys():\n",
        "    denominator = (np.sum(np.array(list(sentimentWordCount[sent].values()))) \\\n",
        "              + (smoothing_factor * len(sentimentWordCount[sent].keys())))\n",
        "    \n",
        "    for word in sentimentWordCount[sent].keys():\n",
        "      numerator = sentimentWordCount[sent][word]\n",
        "      numerator = numerator + smoothing_factor\n",
        "      \n",
        "      cond_prob[sent][word] = numerator / denominator\n",
        "\n",
        "  return features, cond_prob, prior\n",
        "\n",
        "\n",
        "def predict_NB(features, cond_prob, prior, review, stem_func = lambda x:x, n_grams = [1]):\n",
        "  score = {}\n",
        "  words = []\n",
        "\n",
        "  vocab = defaultdict(list)\n",
        "\n",
        "  for sentence in review[\"content\"]:\n",
        "    unigrams = [word[0] for word in sentence]\n",
        "    for n in n_grams:\n",
        "      tokens = ngrams(unigrams, n)\n",
        "      token_list = [] \n",
        "      for token in tokens:\n",
        "        stem_token = stem_func(\" \".join(token).lower())\n",
        "        token_list.append(stem_token)\n",
        "      vocab[n] += token_list\n",
        "\n",
        "  for n in vocab:\n",
        "    for word in vocab[n]:\n",
        "      if(word in features.keys()):\n",
        "        words.append(word)\n",
        "\n",
        "  for cls in cond_prob:\n",
        "    score[cls] = np.log(prior[cls])\n",
        "    for word in words:\n",
        "      if(word in cond_prob[cls]):\n",
        "        score[cls]+= np.log(cond_prob[cls][word])\n",
        "  \n",
        "  return max(score, key=score.get)\n",
        "\n",
        "\n",
        "def test_NB_batch(features, cond_prob, prior, reviews, stem_func = lambda x:x, n_grams = [1]):\n",
        "  scores = []\n",
        "  if(len(reviews) <= 0):\n",
        "    return 0\n",
        "\n",
        "  for review in reviews:\n",
        "    if predict_NB(features, cond_prob, prior, review, stem_func, n_grams) == review[\"sentiment\"]:\n",
        "      scores.append(1)\n",
        "    else:\n",
        "      scores.append(0)\n",
        "  return (np.sum(scores)*100/len(scores))"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set, test_set, class_freq = separateData(reviews)\n",
        "features, cond_prob, prior = trainNBClassifier(training_set, class_freq, smoothing = False)\n",
        "\n",
        "print(f\"Accuracy: {test_NB_batch(features, cond_prob, prior, test_set)}%\")\n",
        "print(f\"Number of Features {len(features.keys())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CwFg-qMNZKa",
        "outputId": "2218edda-d383-4fc7-cb9b-e2350ad0b591"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 83.5%\n",
            "Number of Features 18799\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0INK-PBoM6CB"
      },
      "source": [
        "#### (Q2.3) Would you consider accuracy to also be a good way to evaluate your classifier in a situation where 90% of your data instances are of positive movie reviews? (1pt)\n",
        "\n",
        "Simulate this scenario by keeping the positive reviews\n",
        "data unchanged, but only using negative reviews cv000–cv089 for\n",
        "training, and cv900–cv909 for testing. Calculate the classification\n",
        "accuracy, and explain what changed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFbcsYlipBAw"
      },
      "source": [
        "In the case where 90% of our data instances are of positive movie reviews, accuracy would not be a good measure to evaluate our classifier because the two classes (positive and negative reviews) are unbalanced. So, if in 1000 reviews only 100 are negative and we have a classifier that classsifies every review as positive then this classifier will have 900 true positives and only 100 false positives, so eventually we will have an accuracy of 90% but this would obviously be a bad classifier. This shows that in the case where we want to discover something rare, accuracy is not a good metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWDkt5ZrrFGp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a12094b6-c4cc-4c14-8235-f7ed88ff6dd5"
      },
      "source": [
        "training_set, test_set, class_freq = separateData(reviews, training_idx_start = 0, training_idx_end = 89, testing_idx_start = 900, testing_idx_end = 909)\n",
        "features, cond_prob, prior = trainNBClassifier(training_set, class_freq, smoothing = False)\n",
        "print(f\"Accuracy: {test_NB_batch(features, cond_prob, prior, test_set)}%\")\n",
        "print(f\"Number of Features {len(features.keys())}\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 65.0%\n",
            "Number of Features 4385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wJzcHX3WUDm"
      },
      "source": [
        "## Smoothing (1pt)\n",
        "\n",
        "As mentioned above, the presence of words in the test dataset that\n",
        "have not been seen during training can cause probabilities in the Naive\n",
        "Bayes classifier to be $0$, thus making that particular test instance\n",
        "undecidable. The standard way to mitigate this effect (as well as to\n",
        "give more clout to rare words) is to use smoothing, in which the\n",
        "probability fraction\n",
        "$$\\frac{\\text{count}(w_i, c)}{\\sum\\limits_{w\\in V} \\text{count}(w, c)}$$ for a word\n",
        "$w_i$ becomes\n",
        "$$\\frac{\\text{count}(w_i, c) + \\text{smoothing}(w_i)}{\\sum\\limits_{w\\in V} \\text{count}(w, c) + \\sum\\limits_{w \\in V} \\text{smoothing}(w)}$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBNIcbwUWphC"
      },
      "source": [
        "#### (Q2.4) Implement Laplace feature smoothing (1pt)\n",
        "Implement Laplace smoothing, i.e., smoothing with a constant value ($smoothing(w) = \\kappa, \\forall w \\in V$), in your Naive\n",
        "Bayes classifier’s code, and report the impact on performance. \n",
        "Use $\\kappa = 1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g03yflCc9kpW",
        "outputId": "e93d4fcf-3ab1-433d-dad9-2ed7e3280dd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "training_set, test_set, class_freq = separateData(reviews)\n",
        "features, cond_prob, prior = trainNBClassifier(training_set, class_freq, smoothing = True)\n",
        "print(f\"Accuracy: {test_NB_batch(features, cond_prob, prior, test_set)}%\")\n",
        "print(f\"Number of Features {len(features.keys())}\")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 82.5%\n",
            "Number of Features 45348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiGcgwba87D5"
      },
      "source": [
        "## Cross-Validation (1.5pts)\n",
        "\n",
        "A serious danger in using Machine Learning on small datasets, with many\n",
        "iterations of slightly different versions of the algorithms, is ending up with Type III errors, also called the “testing hypotheses\n",
        "suggested by the data” errors. This type of error occurs when we make\n",
        "repeated improvements to our classifiers by playing with features and\n",
        "their processing, but we don’t get a fresh, never-before seen test\n",
        "dataset every time. Thus, we risk developing a classifier that gets better\n",
        "and better on our data, but only gets worse at generalizing to new, unseen data. In other words, we risk developping a classifier that overfits.\n",
        "\n",
        "A simple method to guard against Type III errors is to use\n",
        "Cross-Validation. In **N-fold Cross-Validation**, we divide the data into N\n",
        "distinct chunks, or folds. Then, we repeat the experiment N times: each\n",
        "time holding out one of the folds for testing, training our classifier\n",
        "on the remaining N - 1 data folds, and reporting performance on the\n",
        "held-out fold. We can use different strategies for dividing the data:\n",
        "\n",
        "-   Consecutive splitting:\n",
        "  - cv000–cv099 = Split 1\n",
        "  - cv100–cv199 = Split 2\n",
        "  - etc.\n",
        "  \n",
        "-   Round-robin splitting (mod 10):\n",
        "  - cv000, cv010, cv020, … = Split 1\n",
        "  - cv001, cv011, cv021, … = Split 2\n",
        "  - etc.\n",
        "\n",
        "-   Random sampling/splitting\n",
        "  - Not used here (but you may choose to split this way in a non-educational situation)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OeLcbSauGtR"
      },
      "source": [
        "#### (Q2.5) Write the code to implement 10-fold cross-validation using round-robin splitting for your Naive Bayes classifier from Q2.4 and compute the 10 accuracies. Report the final performance, which is the average of the performances per fold. If all splits perform equally well, this is a good sign. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KeCGPa7Nuzx"
      },
      "source": [
        "def cross_val(reviews, fold_size =10, stem_func = lambda x:x):\n",
        "  \n",
        "  class_freq = {}\n",
        "  k_folds = []\n",
        "  accuracies = []\n",
        "  var_acc = []\n",
        "  acc_sum = 0\n",
        "\n",
        "  for i in range(fold_size):\n",
        "    split = []\n",
        "    for j, review in enumerate(reviews):\n",
        "        if j%fold_size == i:\n",
        "         split.append(review)\n",
        "         if(review[\"sentiment\"] not in class_freq.keys()):\n",
        "          class_freq[review[\"sentiment\"]] = 1\n",
        "        else:\n",
        "          class_freq[review[\"sentiment\"]]+= 1\n",
        "    k_folds.append(split)  \n",
        "\n",
        "  avg_n_features = 0    \n",
        "\n",
        "  for i in range(fold_size):\n",
        "    print(\"Evaluating Fold \" + str(i) + \"...\")\n",
        "    training_set = []\n",
        "    test_set = []\n",
        "    for j in range(len(k_folds)):\n",
        "      if i == j:\n",
        "        test_set+=k_folds[j]\n",
        "      else:\n",
        "        training_set+=k_folds[j]\n",
        "    features, cond_prob, prior = trainNBClassifier(training_set, class_freq, \\\n",
        "                                smoothing = True, stem_func=stem_func)\n",
        "    acc = test_NB_batch(features, cond_prob, prior, test_set)\n",
        "    acc_sum += acc\n",
        "    print(\"Accuracy for Fold \" + str(i) + \"-> \" + str(acc))\n",
        "    accuracies.append(acc)\n",
        "    avg_n_features += len(features.keys())\n",
        "\n",
        "  avg_n_features/=len(accuracies)\n",
        "  mean_acc = acc_sum / len(accuracies)\n",
        "  var_acc = 0\n",
        "  for acc in accuracies:\n",
        "    var_acc = var_acc + (acc - mean_acc)**2\n",
        "  var_acc/=len(accuracies)\n",
        "  \n",
        "  return accuracies, var_acc, mean_acc, avg_n_features"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies, var_acc, mean_acc, n_features = cross_val(reviews, fold_size =10)\n",
        "print(\"Mean Accuracy: \" + str(mean_acc))"
      ],
      "metadata": {
        "id": "dxBwNU9LgPzz",
        "outputId": "caf38813-4dba-4f0a-dfa6-36e6cf873967",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Fold 0...\n",
            "Accuracy for Fold 0-> 79.0\n",
            "Evaluating Fold 1...\n",
            "Accuracy for Fold 1-> 83.5\n",
            "Evaluating Fold 2...\n",
            "Accuracy for Fold 2-> 80.5\n",
            "Evaluating Fold 3...\n",
            "Accuracy for Fold 3-> 82.5\n",
            "Evaluating Fold 4...\n",
            "Accuracy for Fold 4-> 78.0\n",
            "Evaluating Fold 5...\n",
            "Accuracy for Fold 5-> 84.5\n",
            "Evaluating Fold 6...\n",
            "Accuracy for Fold 6-> 83.0\n",
            "Evaluating Fold 7...\n",
            "Accuracy for Fold 7-> 77.5\n",
            "Evaluating Fold 8...\n",
            "Accuracy for Fold 8-> 83.0\n",
            "Evaluating Fold 9...\n",
            "Accuracy for Fold 9-> 84.0\n",
            "Mean Accuracy: 81.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otdlsDXBNyOa"
      },
      "source": [
        "#### (Q2.6) Report the variance of the 10 accuracy scores. (0.5pt)\n",
        "\n",
        "**Please report all future results using 10-fold cross-validation now\n",
        "(unless told to use the held-out test set).** Note: you're not allowed to use a library for computing the variance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoBQm1KuNzNR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30a606aa-5609-4d97-f5af-9f9f478f26a4"
      },
      "source": [
        "print(f\"Variance -> {var_acc}\")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variance -> 6.022499999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6A2zX9_BRKm"
      },
      "source": [
        "## Features, overfitting, and the curse of dimensionality\n",
        "\n",
        "In the Bag-of-Words model, ideally we would like each distinct word in\n",
        "the text to be mapped to its own dimension in the output vector\n",
        "representation. However, real world text is messy, and we need to decide\n",
        "on what we consider to be a word. For example, is “`word`\" different\n",
        "from “`Word`\", from “`word`”, or from “`words`\"? Too strict a\n",
        "definition, and the number of features explodes, while our algorithm\n",
        "fails to learn anything generalisable. Too lax, and we risk destroying\n",
        "our learning signal. In the following section, you will learn about\n",
        "confronting the feature sparsity and the overfitting problems as they\n",
        "occur in NLP classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKK8FNt8VtcZ"
      },
      "source": [
        "### Stemming (1.5pts)\n",
        "\n",
        "To make your algorithm more robust, use stemming and hash different inflections of a word to the same feature in the BoW vector space. Please use the [Porter stemming\n",
        "    algorithm](http://www.nltk.org/howto/stem.html) from NLTK.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxtCul1IrBi_"
      },
      "source": [
        "#Our naive bayes class already implements this, we only need to initialise our stemmer\n",
        "stemmer = PorterStemmer()"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SrJ1BeLXTnk"
      },
      "source": [
        "#### (Q2.7): How does the performance of your classifier change when you use stemming on your training and test datasets? (1pt)\n",
        "Use cross-validation to evaluate the classifier. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYqKBOiIrInT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aa9de09-74cb-429f-eba3-cb21ba1486d4"
      },
      "source": [
        "#Passing the stemming function already \n",
        "accuracies, var_acc, mean_acc, n_features = cross_val(reviews, fold_size =10, stem_func=stemmer.stem)\n",
        "print(\"Mean Accuracy: \" + str(mean_acc))\n",
        "print(\"Number of features: \" + str(n_features))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Fold 0...\n",
            "Accuracy for Fold 0-> 75.0\n",
            "Evaluating Fold 1...\n",
            "Accuracy for Fold 1-> 80.0\n",
            "Evaluating Fold 2...\n",
            "Accuracy for Fold 2-> 78.0\n",
            "Evaluating Fold 3...\n",
            "Accuracy for Fold 3-> 80.0\n",
            "Evaluating Fold 4...\n",
            "Accuracy for Fold 4-> 77.0\n",
            "Evaluating Fold 5...\n",
            "Accuracy for Fold 5-> 79.5\n",
            "Evaluating Fold 6...\n",
            "Accuracy for Fold 6-> 79.0\n",
            "Evaluating Fold 7...\n",
            "Accuracy for Fold 7-> 76.0\n",
            "Evaluating Fold 8...\n",
            "Accuracy for Fold 8-> 79.5\n",
            "Evaluating Fold 9...\n",
            "Accuracy for Fold 9-> 80.5\n",
            "Mean Accuracy: 78.45\n",
            "Number of features: 32521.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkDHVq_1XUVP"
      },
      "source": [
        "#### (Q2.8) What happens to the number of features (i.e., the size of the vocabulary) when using stemming as opposed to (Q2.4)? (0.5pt)\n",
        "Give actual numbers. You can use the held-out training set to determine these."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA3vee5-rJyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9c72ccd-6ae3-4677-fc89-9e2d20246e49"
      },
      "source": [
        "training_set, test_set, class_freq = separateData(reviews)\n",
        "features, cond_prob, prior = trainNBClassifier(training_set, class_freq, smoothing = False, stem_func=stemmer.stem)\n",
        "print(f\"Accuracy: {test_NB_batch(features, cond_prob, prior, test_set)}%\")\n",
        "print(f\"Number of Features {len(features.keys())}\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 78.0%\n",
            "Number of Features 13326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see how the number of features decreases drastically.\n"
      ],
      "metadata": {
        "id": "81-ypemZn8xa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoazfxbNV5Lq"
      },
      "source": [
        "### N-grams (1.5pts)\n",
        "\n",
        "A simple way of retaining some of the word\n",
        "order information when using bag-of-words representations is to use **n-gram** features. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHjy3I7-qWiu"
      },
      "source": [
        "#### (Q2.9) Retrain your classifier from (Q2.4) using **unigrams+bigrams** and **unigrams+bigrams+trigrams** as features. (1pt)\n",
        "Report accuracy and compare it with that of the approaches you have previously implemented. You are allowed to use NLTK to build n-grams from sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYuKMTOpq9jz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4dc4247-d9c3-4586-eb17-2ade43ceb2e0"
      },
      "source": [
        "n_features = {}\n",
        "print(\"----- Unigrams (Original Implementation) -----\")\n",
        "training_set, test_set, class_freq = separateData(reviews)\n",
        "features, cond_prob, prior = trainNBClassifier(training_set, class_freq, smoothing = False)\n",
        "print(f\"Accuracy: {test_NB_batch(features, cond_prob, prior, test_set)}%\")\n",
        "print(f\"Number of Features {len(features.keys())}\")\n",
        "n_features[\"unigrams\"] = len(features.keys())\n",
        "\n",
        "\n",
        "print(\"----- Bigrams -----\")\n",
        "training_set, test_set, class_freq = separateData(reviews)\n",
        "features, cond_prob, prior = trainNBClassifier(training_set, class_freq, smoothing = False, n_grams=[2])\n",
        "print(f\"Accuracy: {test_NB_batch(features, cond_prob, prior, test_set, n_grams=[2])}%\")\n",
        "print(f\"Number of Features {len(features.keys())}\")\n",
        "\n",
        "\n",
        "print(\"----- Unigrams and Bigrams -----\")\n",
        "training_set, test_set, class_freq = separateData(reviews)\n",
        "features, cond_prob, prior = trainNBClassifier(training_set, class_freq, smoothing = False, n_grams=[1, 2])\n",
        "print(f\"Accuracy: {test_NB_batch(features, cond_prob, prior, test_set, n_grams=[1, 2])}%\")\n",
        "print(f\"Number of Features {len(features.keys())}\")\n",
        "n_features[\"unigrams_bigrams\"] = len(features.keys())\n",
        "\n",
        "print(\"----- Unigrams, Bigrams and Trigrams -----\")\n",
        "features, cond_prob, prior = trainNBClassifier(training_set, class_freq, smoothing = False, n_grams=[1, 2, 3])\n",
        "print(f\"Accuracy: {test_NB_batch(features, cond_prob, prior, test_set, n_grams=[1, 2, 3])}%\")\n",
        "print(f\"Number of Features {len(features.keys())}\")\n",
        "n_features[\"unigrams_bigrams_trigrams\"] = len(features.keys())\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Unigrams (Original Implementation) -----\n",
            "Accuracy: 83.5%\n",
            "Number of Features 18799\n",
            "----- Bigrams -----\n",
            "Accuracy: 84.0%\n",
            "Number of Features 74856\n",
            "----- Unigrams and Bigrams -----\n",
            "Accuracy: 83.5%\n",
            "Number of Features 93655\n",
            "----- Unigrams, Bigrams and Trigrams -----\n",
            "Accuracy: 83.0%\n",
            "Number of Features 156730\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVrGGArkrWoL"
      },
      "source": [
        "\n",
        "#### Q2.10: How many features does the BoW model have to take into account now? (0.5pt)\n",
        "How would you expect the number of features to increase theoretically (e.g., linear, square, cubed, exponential)? How does this number compare, in practice, to the number of features at (Q2.8)?\n",
        "\n",
        "Use the held-out training set once again for this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEGZ9SV8pPaa"
      },
      "source": [
        "We would expect to have a linear increase in our BoW features which is also shown in our feature size plotted below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z8sAJeUrdtM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "a351dbc9-e49c-4794-ae3d-c7afab331565"
      },
      "source": [
        "keys = n_features.keys()\n",
        "values = n_features.values()\n",
        "\n",
        "plt.bar(keys, values, width = 0.4)\n",
        "  \n",
        "plt.xlabel(\"N-Grams Used\") \n",
        "plt.ylabel(\"No. of Features\") \n",
        "plt.title(\"Feature Variation with N-Grams used\") \n",
        "plt.show() "
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHWKDL3YV6vh"
      },
      "source": [
        "# (3) Support Vector Machines (4pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJSYhcVaoJGt"
      },
      "source": [
        "Though simple to understand, implement, and debug, one\n",
        "major problem with the Naive Bayes classifier is that its performance\n",
        "deteriorates (becomes skewed) when it is being used with features which\n",
        "are not independent (i.e., are correlated). Another popular classifier\n",
        "that doesn’t scale as well to big data, and is not as simple to debug as\n",
        "Naive Bayes, but that doesn’t assume feature independence is the Support\n",
        "Vector Machine (SVM) classifier.\n",
        "\n",
        "You can find more details about SVMs in Chapter 7 of Bishop: Pattern Recognition and Machine Learning.\n",
        "Other sources for learning SVM:\n",
        "* http://web.mit.edu/zoya/www/SVM.pdf\n",
        "* http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf\n",
        "* https://pythonprogramming.net/support-vector-machine-intro-machine-learning-tutorial/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Use the scikit-learn implementation of \n",
        "[SVM](http://scikit-learn.org/stable/modules/svm.html) with the default parameters. (You are not expected to perform any hyperparameter tuning, but feel free to do it if you think it gives you good insights for the discussion in question 5.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LnzNtQBV8gr"
      },
      "source": [
        "#### (Q3.1): Train SVM and compare to Naive Bayes (2pts)\n",
        "\n",
        "Train an SVM classifier (sklearn.svm.LinearSVC) using the features collected for Naive Bayes. Compare the\n",
        "classification performance of the SVM classifier to that of the Naive\n",
        "Bayes classifier with smoothing.\n",
        "Use cross-validation to evaluate the performance of the classifiers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBscui8Mvoz0"
      },
      "source": [
        "def separateDataSVM(reviews, training_idx_start = 0, training_idx_end = 899, testing_idx_start = 900, testing_idx_end = 999):\n",
        "  training_set = []\n",
        "  test_set = []\n",
        "  review_content = []\n",
        "   \n",
        "\n",
        "  for review in reviews:\n",
        "    if review[\"cv\"]<= training_idx_end and review[\"cv\"] >= training_idx_start:\n",
        "      training_set.append(review)\n",
        "    elif review[\"cv\"]<= testing_idx_end and review[\"cv\"] >= testing_idx_start:\n",
        "      test_set.append(review)\n",
        "\n",
        "  training_df = DataFrame(training_set, columns=['sentiment', 'content'])\n",
        "  testing_df = DataFrame(test_set, columns=['sentiment','content'])\n",
        "  return training_df, testing_df\n",
        "\n",
        "def preprocess(df, pos, closed_classes):\n",
        "  fixed_set = []\n",
        "  for review in df:\n",
        "    r = \"\"\n",
        "    for sentence in review:\n",
        "      for word, tag in sentence:\n",
        "        if closed_classes:\n",
        "          if not (tag.startswith('NN') or tag.startswith('VB') or tag.startswith('JJ') or tag.startswith('RB')):\n",
        "            continue\n",
        "        if pos:\n",
        "          r += word.lower() + \"_\" + tag + \" \"\n",
        "        else:\n",
        "          r += word.lower() + \" \"\n",
        "    fixed_set.append(r)\n",
        "  fixed_df = [x.lower() for x in fixed_set]\n",
        "  return fixed_df"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def acc_svm(pred, y):\n",
        "  scores = []\n",
        "  if(len(pred) <= 0):\n",
        "    return 0\n",
        "  for i, p in enumerate(pred):\n",
        "    if p == y[i]:\n",
        "      scores.append(1)\n",
        "    else:\n",
        "      scores.append(0)\n",
        "  return (np.sum(scores)*100/len(scores))\n",
        "\n",
        "def cross_val_svm(reviews, fold_size =10, pos=False, closed_classes=False, stem_func = lambda x:x):\n",
        "  k_folds = []\n",
        "  accuracies = []\n",
        "  var_acc = []\n",
        "  acc_sum = 0\n",
        "  cntdf = CountVectorizer(stop_words='english')\n",
        "\n",
        "  for i in range(fold_size):\n",
        "    split = []\n",
        "    for j, review in enumerate(reviews):\n",
        "        if j%fold_size == i:\n",
        "         split.append(review)\n",
        "    k_folds.append(split)      \n",
        "\n",
        "  for i in range(fold_size):\n",
        "    print(\"Evaluating Fold \" + str(i) + \"...\")\n",
        "    training_set = []\n",
        "    test_set = []\n",
        "    for j in range(len(k_folds)):\n",
        "      if i == j:\n",
        "        test_set+=k_folds[j]\n",
        "      else:\n",
        "        training_set+=k_folds[j]\n",
        "    training_df = DataFrame(training_set, columns=['sentiment', 'content'])\n",
        "    testing_df = DataFrame(test_set, columns=['sentiment','content'])\n",
        "\n",
        "    train_x, train_y = training_df['content'], training_df['sentiment']\n",
        "    test_x, test_y = testing_df['content'], testing_df['sentiment']\n",
        "\n",
        "    train_x_svm = preprocess(train_x, pos, closed_classes)\n",
        "    test_x_svm = preprocess(test_x, pos, closed_classes)\n",
        "    \n",
        "    train_vectors = cntdf.fit_transform(train_x_svm)\n",
        "    test_vectors = cntdf.transform(test_x_svm)\n",
        "\n",
        "    clf = svm.SVC()\n",
        "    clf.fit(train_vectors, train_y)\n",
        "\n",
        "    pred = clf.predict(test_vectors)\n",
        "\n",
        "    acc = acc_svm(pred, test_y)\n",
        "    acc_sum += acc\n",
        "    accuracies.append(acc)\n",
        "\n",
        "  mean_acc = acc_sum / len(accuracies)\n",
        "  var_acc = 0\n",
        "  for acc in accuracies:\n",
        "    var_acc = var_acc + (acc - mean_acc)**2\n",
        "  var_acc/=len(accuracies)\n",
        "  \n",
        "  return accuracies, var_acc, mean_acc"
      ],
      "metadata": {
        "id": "F9uy-RPfYWo4"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc, var_acc, mean_acc = cross_val_svm(reviews)\n",
        "for fold, accuracy in enumerate(acc):\n",
        "  print(f\"Accuracy for fold {fold} -> {accuracy}\")\n",
        "\n",
        "print(\"Mean Accuracy: \" + str(mean_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ezZ7HDCYZ79",
        "outputId": "1769822b-b7ff-47e7-96c7-ca0868e95754"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Fold 0...\n",
            "Evaluating Fold 1...\n",
            "Evaluating Fold 2...\n",
            "Evaluating Fold 3...\n",
            "Evaluating Fold 4...\n",
            "Evaluating Fold 5...\n",
            "Evaluating Fold 6...\n",
            "Evaluating Fold 7...\n",
            "Evaluating Fold 8...\n",
            "Evaluating Fold 9...\n",
            "Accuracy for fold 0 -> 75.0\n",
            "Accuracy for fold 1 -> 85.0\n",
            "Accuracy for fold 2 -> 78.5\n",
            "Accuracy for fold 3 -> 84.0\n",
            "Accuracy for fold 4 -> 77.0\n",
            "Accuracy for fold 5 -> 84.0\n",
            "Accuracy for fold 6 -> 83.5\n",
            "Accuracy for fold 7 -> 81.5\n",
            "Accuracy for fold 8 -> 81.5\n",
            "Accuracy for fold 9 -> 78.0\n",
            "Mean Accuracy: 80.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifXVWcK0V9qY"
      },
      "source": [
        "### POS disambiguation (2pts)\n",
        "\n",
        "Now add in part-of-speech features. You will find the\n",
        "movie review dataset has already been POS-tagged for you ([here](https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf) you find the tagset). Try to\n",
        "replicate the results obtained by Pang et al. (2002).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA3I82o4oWGu"
      },
      "source": [
        "####(Q3.2) Replace your features with word+POS features, and report performance with the SVM. Use cross-validation to evaluate the classifier and compare the results with (Q3.1). Does part-of-speech information help? Explain why this may be the case. (1pt)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOvjYe-t2Br6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "522e5f48-16a9-4948-9cd1-485bdc99b9d7"
      },
      "source": [
        "acc, var_acc, mean_acc = cross_val_svm(reviews, pos=True)\n",
        "for fold, accuracy in enumerate(acc):\n",
        "  print(f\"Accuracy for fold {fold} -> {accuracy}\")\n",
        "\n",
        "print(\"Mean Accuracy: \" + str(mean_acc))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Fold 0...\n",
            "Evaluating Fold 1...\n",
            "Evaluating Fold 2...\n",
            "Evaluating Fold 3...\n",
            "Evaluating Fold 4...\n",
            "Evaluating Fold 5...\n",
            "Evaluating Fold 6...\n",
            "Evaluating Fold 7...\n",
            "Evaluating Fold 8...\n",
            "Evaluating Fold 9...\n",
            "Accuracy for fold 0 -> 73.0\n",
            "Accuracy for fold 1 -> 72.5\n",
            "Accuracy for fold 2 -> 75.5\n",
            "Accuracy for fold 3 -> 72.5\n",
            "Accuracy for fold 4 -> 67.5\n",
            "Accuracy for fold 5 -> 81.0\n",
            "Accuracy for fold 6 -> 76.0\n",
            "Accuracy for fold 7 -> 72.0\n",
            "Accuracy for fold 8 -> 77.0\n",
            "Accuracy for fold 9 -> 74.5\n",
            "Mean Accuracy: 74.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0dt_oQupUNe"
      },
      "source": [
        "We can see that our classifier performs worse when taking into account POS tags for words, we belive this is the case as incorporating POS tags does not add any relevant information that will help our model classify the reviews better. What we can experience when taking into account POS tags is an increase of words in our vocabulary with generally lower frequencies caused by words being used in many different forms in our reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su-3w87eMW0w"
      },
      "source": [
        "#### (Q3.3) Discard all closed-class words from your data (keep only nouns, verbs, adjectives, and adverbs), and report performance. Does this help? Use cross-validation to evaluate the classifier and compare the results with (Q3.2). Are closed-class words detrimental to the classifier? Explain why this may be the case. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCUPlPozCYUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab85fbc1-bf81-4110-8480-e00cd7bef00e"
      },
      "source": [
        "acc, var_acc, mean_acc = cross_val_svm(reviews, closed_classes=True)\n",
        "for fold, accuracy in enumerate(acc):\n",
        "  print(f\"Accuracy for fold {fold} -> {accuracy}\")\n",
        "\n",
        "print(\"Mean Accuracy: \" + str(mean_acc))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Fold 0...\n",
            "Evaluating Fold 1...\n",
            "Evaluating Fold 2...\n",
            "Evaluating Fold 3...\n",
            "Evaluating Fold 4...\n",
            "Evaluating Fold 5...\n",
            "Evaluating Fold 6...\n",
            "Evaluating Fold 7...\n",
            "Evaluating Fold 8...\n",
            "Evaluating Fold 9...\n",
            "Accuracy for fold 0 -> 77.0\n",
            "Accuracy for fold 1 -> 82.5\n",
            "Accuracy for fold 2 -> 80.0\n",
            "Accuracy for fold 3 -> 82.5\n",
            "Accuracy for fold 4 -> 78.0\n",
            "Accuracy for fold 5 -> 83.5\n",
            "Accuracy for fold 6 -> 83.5\n",
            "Accuracy for fold 7 -> 83.0\n",
            "Accuracy for fold 8 -> 84.5\n",
            "Accuracy for fold 9 -> 79.5\n",
            "Mean Accuracy: 81.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaxCVrs8pWSp"
      },
      "source": [
        "Discarding closed-class words from our data doesn't enhance the performance of our classifier drastically. More specifically, we observe a slight (0.6%) increase in it's performance. This is the case because it's the open-class words that mostly express emotion. Although, using both closed-class and open-class words result in better classification performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfwqOciAl2No"
      },
      "source": [
        "# (Q4) Discussion (max. 500 words). (5pts)\n",
        "\n",
        "> Based on your experiments, what are the effective features and techniques in sentiment analysis? What information do different features encode?\n",
        "Why is this important? What are the limitations of these features and techniques?\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYuse5WLmekZ"
      },
      "source": [
        "To do sentiment classification for movie reviews we implemented a Lexicon-based approach, a Naive Bayes approach and an SVM approach. With the lexicon-based approach, we achieved an accuracy of 67%, although we managed to get a slight increase of 1% when incorporating the magnitude of the sentiment. The accuracy achieved is quite low compared to the other approaches. This might be the case because while the lexicon-based approach is intuitive and relatively easy to implement it only evaluates individual words that exist in the lexicon and also ignores the context in which each word is encountered.\n",
        "\n",
        "The Naive Bayes classifier achieved an accuracy of 83.5%. This is significantly better than the 67% achieved by the lexicon-based classifier and it happens probably because it is a probabilistic classifier that incorporates prior knowledge and uses conditional probabilities of a word occurring in a positive or negative review. It assumes that features are independent so their relative position in a sentence is not taken into account. Lack of sufficient training data can lead to poor classification in case of unseen words. Poor classification might also occur in the case of unbalanced data. To address this problem an efficient technique to use is smoothing. In addition, to make sure our classifier generalizes well on new data we can use n-fold cross-validation. In our implementation, we achieved an average accuracy of all folds of 81.55% which is very close to the accuracy achieved without it. To reduce the size of our vocabulary to not include words with the same stem and account for them as one word we used stemming but this resulted in worse results either because the stemmer was not that good or because the extra words in our vocabulary were indeed helpful. Finally, to capture context (consecutive words) in our reviews we used n-grams. Specifically, we got a slight increase when only using bigrams leading to an accuracy of 84% while we got the same accuracy of 83.5% when using unigrams and bigrams. This means that the use of n-grams can be helpful but they don't always manage to capture the context we are looking for.\n",
        "\n",
        "Finally, the SVM approach led to an accuracy of 80.7%. This approach is easy to implement and does an adequate job in sentiment classification. Incorporating POS tags doesn't make a big difference because they don't provide any useful semantic information for the SVM classifier. Finally discarding closed-class words can increase the performance since the sentiment of reviews is mostly captured in the open-class words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwaKwfWQhRk_"
      },
      "source": [
        "# Submission \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOUeaET5ijk-"
      },
      "source": [
        "# Write your names and student numbers here:\n",
        "# Gerard Planella Fontanillas #14244950\n",
        "# Chrysoula Pozrikidou #14609797"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A9K-H6Tii3X"
      },
      "source": [
        "**That's it!**\n",
        "\n",
        "- Check if you answered all questions fully and correctly. \n",
        "- Download your completed notebook using `File -> Download .ipynb` \n",
        "- Check if your answers are all included in the file you submit.\n",
        "- Submit your .ipynb file via *Canvas*. One submission per group. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHslatYAKBrF"
      },
      "source": [],
      "execution_count": 70,
      "outputs": []
    }
  ]
}